
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Flights at ABIA
```{r}
library(ggplot2)
par(mfrow=c(2,2))
data=read.csv('C:/Users/rache/OneDrive/Documents/MSBA/Academic/next session/STA380-master/STA380-master/data/ABIA.csv')
data$Austin=ifelse(data$Origin=='AUS',1,0)
```

# What is the best time of day to fly to minimize delays?
```{r}
data$hour=round(data$DepTime/100)
plot(data$hour, data$ArrDelay/60, pch=19, col=data$hour, xlab='Departure Time', ylab='ArrDelay (in hours)', main='ArrDelays by Departure Time')
median_day=rep(NA,24)
std_day=rep(NA, 24)
for(i in 1:24){
  newsub=subset(data, hour==i)
  median_day[i]<-median(newsub$ArrDelay/60, na.rm=TRUE)
  std_day[i]<-sd(newsub$ArrDelay/60, na.rm=TRUE)
  
}
choose_day=data.frame(col1=median_day,col2=std_day)
choose_day[
  with(choose_day, order(choose_day$col2),decreasing=FALSE),
]
```
The table above indicates the standard deviation and median of different Arrival Delays in hours in each departuring hour. Here, we could see that 5 am has the lowest standard deviation and low median.
## What is the best day of week to fly to minimize delays?
```{r}
plot(data$DayOfWeek, data$ArrDelay/60, pch=19, col=data$DayOfWeek, xlab='Departure Month', ylab='ArrDelay in hours', main='ArrDelays by Day in a Week')
median_week=rep(NA,7)
std_week=rep(NA, 7)
for(i in 1:7){
  newsub=subset(data, DayOfWeek==i)
  median_week[i]<-median(newsub$ArrDelay/60, na.rm=TRUE)
  std_week[i]<-sd(newsub$ArrDelay/60, na.rm=TRUE)
  
}
choose_week=data.frame(col1=median_week,col2=std_week)
choose_week[
  with(choose_week, order(choose_week$col2),decreasing=FALSE),
]
```
The table above indicates the standard deviation and median of different Arrival Delays in hours in each day of the week. Here, we could see that Wednesday has the lowest standard deviation and low median.
## What is the best month of year to fly to minimize delays?
```{r}
plot(data$Month, data$ArrDelay/60, pch=19, col=data$Month, xlab='Departure Month', ylab='ArrDelay in hours', main='ArrDelays by Month in a Year')
median_month=rep(NA,12)
std_month=rep(NA, 12)
for(i in 1:12){
  newsub=subset(data, Month==i)
  median_month[i]<-median(newsub$ArrDelay/60, na.rm=TRUE)
  std_month[i]<-sd(newsub$ArrDelay/60, na.rm=TRUE)
  
}
choose_month=data.frame(col1=median_month,col2=std_month)
choose_month[
  with(choose_month, order(choose_month$col2),decreasing=FALSE),
]
```
The table above indicates the standard deviation and median of different Arrival Delays in hours in each Month. Here, we could see that september has the lowest standard deviation and low median.
## What are the bad airports to fly to?
```{r}
airport=aggregate(data$ArrDelay, by=list(Category=data$Dest), FUN=mean, na.rm='True')
airport=airport[order(airport$x,decreasing = TRUE),]
airport=subset(airport,x>10)
ggplot(aes(x = Category, y = x), data = airport ) +geom_bar(stat = "identity") + xlab("Destination Airport") + 
  ylab("Average Delay in minutes") + ggtitle("Destinatin Airports with delays")
```
From the all the plots above, we can see that the best time to fly to minimize delay time is round 5 in the morning on Wednesday during September. \\
In addition, we found that the Des Moines International Airport (DSM) airport has the most delay time as a destination airport from Austin Airport.\\

##How do patterns of flights to different destinations or parts of the country change over the course of the year?

Here, we could take a look at the pattern of the flight (to Austin or from Austin in January, June and December)

```{r}
library(igraph)
index=c(2,17,18)
network_flight=data[index]
```


```{r}
##looking at January 
network_1=subset(network_flight, Month==1 )
index_1=c(2,3)
network_1=network_1[index_1]
network_1=as.matrix(network_1)
#####creating a graph object####
graph_network_1=graph.edgelist(network_1, directed=FALSE)
sort(degree(graph_network_1),decreasing = TRUE)
#start ploting the network

V(graph_network_1)$color = "orange"
V(graph_network_1)$frame.color = 0
V(graph_network_1)$label.color = "black"
plot(graph_network_1, edge.curved=FALSE)
```
```{r}
##looking at June  
network_6=subset(network_flight, Month==6 )
index_1=c(2,3)
network_6=network_6[index_1]
network_6=as.matrix(network_6)
#####creating a graph object####
graph_network_6=graph.edgelist(network_6, directed=FALSE)
sort(degree(graph_network_6),decreasing = TRUE)
#start ploting the network

V(graph_network_6)$color = "orange"
V(graph_network_6)$frame.color = 0
V(graph_network_6)$label.color = "black"
plot(graph_network_6, edge.curved=FALSE)
```

```{r}
##looking at December  
network_12=subset(network_flight, Month==12 )
index_1=c(2,3)
network_12=network_12[index_1]
network_12=as.matrix(network_12)
#####creating a graph object####
graph_network_12=graph.edgelist(network_12, directed=FALSE)
sort(degree(graph_network_12),decreasing = TRUE)
#start ploting the network

V(graph_network_12)$color = "orange"
V(graph_network_12)$frame.color = 0
V(graph_network_12)$label.color = "black"
plot(graph_network_12, edge.curved=FALSE)
```
From the graph, we can see that all other flights flew frequently from or too Austin except TYS, SAT, MSP, BHM in January, and the top 5 airports having high degree with Austin are DAL, DFW, IAH, PHX, and ATL. The top 5 airports having high degree with Austin in June are DAL, DFW, IAH, DEN, and PHX. The top 5 airports having high degree with Austin in December are DFW, DAL, IAH, PHX, and DEN



## Author attribution
```{r}
library(tm)
library(foreach)
library(randomForest)
library(glmnet)
library(tidyverse)

set.seed("6")

readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), 
            id=fname, language='en') }
#read in train and test
train_list = Sys.glob('ReutersC50/C50train/*/*.txt')
test_list = Sys.glob('ReutersC50/C50test/*/*.txt')
file_list = c(train_list,test_list)

alldata = lapply(file_list, readerPlain) 

filename = file_list %>%
{ strsplit(., '/', fixed=TRUE) } %>%
{ lapply(., tail, n=2) } %>%
{ lapply(., paste0, collapse = '') } %>%
  unlist

authorname = file_list %>%
{ strsplit(., '/', fixed=TRUE) } %>%
{ lapply(., tail, n=2) } %>%
{ lapply(., head, n=1) } %>% 
  unlist

names(alldata) = filename

documents_raw = Corpus(VectorSource(alldata))
#tokenization process
my_documents = documents_raw
my_documents = tm_map(my_documents, content_transformer(tolower))
my_documents = tm_map(my_documents, content_transformer(removeNumbers))
my_documents = tm_map(my_documents, content_transformer(removePunctuation))
my_documents = tm_map(my_documents, content_transformer(stripWhitespace))
my_documents = tm_map(my_documents, content_transformer(removeWords), stopwords("en"))
#create document term matrix and remove sparcity
DTM_all = DocumentTermMatrix(my_documents)
DTM_all = removeSparseTerms(DTM_all, 0.95)

tfidf_all = weightTfIdf(DTM_all)

X = as.matrix(tfidf_all)
summary(colSums(X))
scrub_cols = which(colSums(X) == 0)
X = X[,-scrub_cols]
#conduct principle component analysis
pca_train = prcomp(X, scale=TRUE)

X = pca_train$x[1:2500,1:100]
y = authorname[1:2500]
```

```{r}
#Model1: lasso
out1 = cv.glmnet(X, y, family='multinomial', type.measure="class")

lambda_hat = out1$lambda.min

paste("The chosen lambda is",lambda_hat)

predict.lasso = predict.cv.glmnet(out1,pca_train$x[2501:5000,1:100],s=lambda_hat)

predict.name = vector()
for (i in 1:2500){
  a=which(predict.lasso[i,,1] == max(predict.lasso[i,,1]))
  predict.name = c(predict.name,colnames(predict.lasso)[a])
  
}

result = predict.name == authorname[2501:5000]
paste("The correction ratio is",sum(result) / length(result))

a=table(predict.name, authorname[2501:5000])
a[1:10,1:10]
```




```{r }
#Model2:Random Forest
############
p=2500-1
mtryv = sqrt(p)
ntreev = 600

a=data.frame(authorname[1:2500],pca_train$x[1:2500,1:50])
colnames(a)[1]="author"

temprf = randomForest(author ~., data=a,mtry=mtryv,ntree=ntreev)
predict.name = predict(temprf,newdata=pca_train$x[2501:5000,1:50])
result = predict.name == authorname[2501:5000]
paste("The correction ratio is", sum(result) / length(result))

t=table(predict.name, authorname[2501:5000])
t[1:10,1:10]

k=0
i1=0
j1=0
for (i in 1:50) {
  for (j in 1:50) {
      if(i != j){
        if(t[i,j]>k){
          k=t[i,j]
          i1=i
          j1=j
        }
      }
  }
}

t[12:14,6:8]

```

From the result we can see that the Lasso regression model is better than random forest. DarrenSchuettler and HeatherScoffield are most easily be distinguished.





## Practice with association rule mining
```{r}
library(arules)
library(arulesViz)
library(igraph) 
grocery=read.transactions("groceries.txt", format="basket", sep=",")
#grocery_graph=as.matrix(read.transactions("groceries.txt"))
itemFrequencyPlot(grocery,topN=20,type="absolute",col='blue',xlab='Item',main='Frequency of Item Purchased')

## Cast this variable as a special arules "transactions" class.
gro_trans = as(grocery, "transactions")
summary(gro_trans)
gro_rules = apriori(gro_trans,parameter=list(support=.005, confidence=.5, maxlen=6))

inspect(gro_rules)

inspect(subset(gro_rules, subset=lift > 3))
inspect(subset(gro_rules, subset=confidence > 0.5))
```
From the inspect, we can see that other vegetables and whole milk are the items mostly purchased and they are the items with highest support values. We can also find the two kinds of item are most frequently purchased in the plot of the Top 20 frequency of item purchased.\\
We can see that there are associations between people who buy a certain kind of items and also buy some of the more frequently purchased items. For instance, people who buy onion or root vegetables also tend to purchase other vegetables. And customers who buy diary products will also be more likely to buy whole milk.\\




